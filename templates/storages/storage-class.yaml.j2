## curl -L https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml > storage_class_csi.yaml
## curl -L https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/csi/rbd/storageclass-ec.yaml >> storage_class_csi.yaml

#################################################################################################################
# Create a storage class with a data pool that uses erasure coding for a production environment.
# A metadata pool is created with replication enabled. A minimum of 3 nodes with OSDs are required in this
# example since the default failureDomain is host.
#  kubectl create -f storageclass-ec.yaml
#################################################################################################################

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: {{ item.name }}
provisioner: {{ item.storageClassProvisioner }}
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the namespace secrets are defined
  clusterID: {{ rook_operator_namespace }}

{% if item.storageClassProvisioner == 'rook-ceph.cephfs.csi.ceph.com' %}
  # CephFS filesystem name into which the volume shall be created
  fsName: {{ item.fileSystemName }}

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: {{ item.fileSystemName }}-data0

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  # rootPath: /absolute/path

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_operator_namespace }}

  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
  # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
  # or by setting the default mounter explicitly via --volumemounter command-line argument.
  # mounter: kernel
{% elif item.storageClassProvisioner == 'rook-ceph.rbd.csi.ceph.com' %}
  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the `pool` parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the `dataPool` parameter below.
  # Ceph pool into which the RBD image shall be created
  dataPool: {{ item.dataBlockPoolName }}
  pool: {{ item.metaBlockPoolName }}

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
  imageFeatures: layering

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace:  {{ rook_operator_namespace }}
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace:  {{ rook_operator_namespace }}
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  #csi.storage.k8s.io/fstype: xfs
# uncomment the following to use rbd-nbd as mounter on supported nodes
# **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi
# issue that causes the mount to be disconnected. You will need to follow special upgrade steps
# to restart your application pods. Therefore, this option is not recommended.
#mounter: rbd-nbd
allowVolumeExpansion: true
{% endif %}
  # The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
  #clusterNamespace: {{ rook_cluster_namespace }}
  # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
  #mountUser: user1
  # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
  # The secret must exist in each namespace(s) where the storage will be consumed.
  #mountSecret: ceph-user1-secret

# Optional, default reclaimPolicy is "Delete". Other options are:
# "Retain", "Recycle" as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/
reclaimPolicy: {{ item.reclaimPolicy | default('Retain') }}
{% if item.mountOptions is defined %}
mountOptions: {{ item.mountOptions }}
  # uncomment the following line for debugging
  #- debug
{% endif %}
