## curl -L https://raw.githubusercontent.com/rook/rook/v1.3.2/cluster/examples/kubernetes/ceph/csi/cephfs/storageclass.yaml > storageclass-cephfs.yaml
## curl -L https://raw.githubusercontent.com/rook/rook/v1.3.2/cluster/examples/kubernetes/ceph/csi/rbd/storageclass-ec.yaml > storageclass-rbd-ec.yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: {{ item.name }}
provisioner: {{ item.storageClassProvisioner }}
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: {{ rook_operator_namespace }}

{% if item.storageClassProvisioner == 'rook-ceph.cephfs.csi.ceph.com' %}
  # CephFS filesystem name into which the volume shall be created
  fsName: {{ item.fileSystemName }}

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: {{ item.fileSystemName }}-data

  # Root path of an existing CephFS volume
  # Required for provisionVolume: "false"
  # rootPath: /absolute/path

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: {{ rook_operator_namespace }}

  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
  # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
  # or by setting the default mounter explicitly via --volumemounter command-line argument.
  # mounter: kernel
{% elif item.storageClassProvisioner == 'rook-ceph.rbd.csi.ceph.com' %}
  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the `pool` parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the `dataPool` parameter below.
  # Ceph pool into which the RBD image shall be created
{% if item.metaBlockPoolName is defined %}
  dataPool: {{ item.dataBlockPoolName }}
  pool: {{ item.metaBlockPoolName }}
{% else %}
  pool: {{ item.dataBlockPoolName }}
{% endif %}

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
  imageFeatures: {{ item.imageFeatures | default('layering') }}

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace:  {{ rook_operator_namespace }}
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: {{ rook_operator_namespace }}
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace:  {{ rook_operator_namespace }}
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  csi.storage.k8s.io/fstype: {{ item.fstype | default('ext4') }}
# uncomment the following to use rbd-nbd as mounter on supported nodes
# **IMPORTANT**: If you are using rbd-nbd as the mounter, during upgrade you will be hit a ceph-csi
# issue that causes the mount to be disconnected. You will need to follow special upgrade steps
# to restart your application pods. Therefore, this option is not recommended.
#mounter: rbd-nbd
allowVolumeExpansion: true
{% endif %}

# Optional, default reclaimPolicy is "Delete". Other options are:
# "Retain", "Recycle" as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/
reclaimPolicy: {{ item.reclaimPolicy | default('Retain') }}
{% if item.mountOptions is defined %}
mountOptions: {{ item.mountOptions }}
  # uncomment the following line for debugging
  #- debug
{% endif %}
