## See https://github.com/rook/rook/blob/v1.3.2/cluster/examples/kubernetes/ceph/common.yaml#L505

apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: {{ item.name }}
  namespace: {{ rook_cluster_namespace }}
spec:
  # The failure domain will spread the replicas of the data across different failure zones
  failureDomain: {{ item.failureDomain | default('host') }}
{% if item.erasureCoded is defined %}
  # Make sure you have enough OSDs to support the replica size or sum of the erasure coding and data chunks.
  erasureCoded:
    dataChunks: {{ item.erasureCoded.dataChunks | default(2) }}
    codingChunks: {{ item.erasureCoded.codingChunks | default(1) }}
{% elif item.replicated is defined %}
  # For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.
  replicated:
    size: {{ item.replicated.size | mandatory }}
    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
    requireSafeReplicaSize: {% if item.replicated.size | default(2) == 1 %}false{% else %}{{ item.replicated.requireSafeReplicaSize | default(true) }}{% endif %}

{% if item.targetSizeRatio is defined %}
    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
    targetSizeRatio: {{ item.targetSizeRatio }}
{% endif %}
{% if item.requireSafeReplicaSize is defined %}
    requireSafeReplicaSize: {{ item.requireSafeReplicaSize }}
{% endif %}
{% else %}
{{ item.replicated | mandatory }}
{% endif %}
  # Inline compression mode for the data pool
  compressionMode: {{ item.compressionMode | default('none') }}
{% if item.annotations is defined %}
  # A key/value list of annotations
  annotations:
    {{ item.annotations | to_nice_yaml | indent(4, indentfirst=false) }}
{% endif %}
