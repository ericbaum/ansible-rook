## curl -L https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/flex/storageclass.yaml > storage_class.yaml

#################################################################################################################
# Create a storage class with a pool that sets replication for a production environment.
# A minimum of 3 nodes with OSDs are required in this example since the default failureDomain is host.
#  kubectl create -f storageclass.yaml
#################################################################################################################

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: {{ item.name }}
provisioner: ceph.rook.io/{{ item.storage_class_provisioner }}
parameters:
{% if item.storage_class_provisioner == 'block' %}
  blockPool: {{ item.block_pool_name }}
  # Specify the filesystem type of the volume. If not specified, it will use `ext4`.
  fstype: {{ item.fstype | default('xfs') }}
{% endif %}
{% if item.storage_class_provisioner == 'filesystem' %}
  fsName: {{ item.file_system_name }}
{% endif %}
  # The value of "clusterNamespace" MUST be the same as the one in which your rook cluster exist
  clusterNamespace: {{ rook_cluster_namespace }}
  # (Optional) Specify an existing Ceph user that will be used for mounting storage with this StorageClass.
  #mountUser: user1
  # (Optional) Specify an existing Kubernetes secret name containing just one key holding the Ceph user secret.
  # The secret must exist in each namespace(s) where the storage will be consumed.
  #mountSecret: ceph-user1-secret
# Optional, default reclaimPolicy is "Delete". Other options are: "Retain", 
# "Recycle" as documented in https://kubernetes.io/docs/concepts/storage/storage-classes/
reclaimPolicy: Retain
